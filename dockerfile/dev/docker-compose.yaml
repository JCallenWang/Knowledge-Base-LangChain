# the version of docker compose parser 
version: "3.9"

services:
  # service name
  ollama-langchain:
    # image name
    image: fc-langchain-rag:v0.1.1
    container_name: fc-rag-dev
    ports:
      - "11450:11434"
    volumes:
      - ../../workspace:/workspace
      - ollama:/root/.ollama
    # below only work in Docker Swarm mode
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: all
    #        - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
    runtime: nvidia
    tty: true
    stdin_open: true

volumes:
  ollama:               # the name of volume
    external: true      # true if exisit, false for creating automatically
